{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# S&P 500 Price Analysis and Prediction ðŸ“ˆ\n\nIn this notebook, I will use **Artificial Intelligence** to analyze historical data from the **S&P 500 (^GSPC)** index and attempt to predict price movements. \n\nThe goal is to move from simple data visualization to a robust **Machine Learning** model, identifying patterns and trends over the last 10 years.","metadata":{}},{"cell_type":"code","source":"import yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Graphic's style\nplt.style.use('fivethirtyeight')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:14.616874Z","iopub.execute_input":"2026-02-25T16:02:14.617772Z","iopub.status.idle":"2026-02-25T16:02:14.623065Z","shell.execute_reply.started":"2026-02-25T16:02:14.617737Z","shell.execute_reply":"2026-02-25T16:02:14.621813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fetching data\ndf = yf.download('^GSPC', start='2014-01-01', end='2024-12-31', auto_adjust=True)\n\n# FIX: If columns are MultiIndex (common in new yfinance versions), flatten them\nif isinstance(df.columns, pd.MultiIndex):\n    df.columns = df.columns.get_level_values(0)\n\n# Displaying the first rows to confirm\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:14.624832Z","iopub.execute_input":"2026-02-25T16:02:14.625329Z","iopub.status.idle":"2026-02-25T16:02:14.698103Z","shell.execute_reply.started":"2026-02-25T16:02:14.625288Z","shell.execute_reply":"2026-02-25T16:02:14.696962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting the historical closing price\nplt.figure(figsize=(16,8))\nplt.title('S&P 500 Historical Closing Price')\nplt.plot(df['Close'])\nplt.xlabel('Date')\nplt.ylabel('Price USD ($)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:14.699393Z","iopub.execute_input":"2026-02-25T16:02:14.699670Z","iopub.status.idle":"2026-02-25T16:02:14.941603Z","shell.execute_reply.started":"2026-02-25T16:02:14.699645Z","shell.execute_reply":"2026-02-25T16:02:14.940522Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Technical Indicators: Moving Averages\n\nThe crossover between the **50-day** and **200-day Moving Averages (MA)** is a classic indicator in financial markets. \n\n* **Golden Cross:** When the 50-day MA crosses above the 200-day MA (Bullish signal).\n* **Death Cross:** When the 50-day MA crosses below the 200-day MA (Bearish signal).","metadata":{}},{"cell_type":"code","source":"# 50-day Moving Average (short/medium term)\ndf['MA50'] = df['Close'].rolling(window=50).mean()\n\n# 200-day Moving Average (long term - crucial for investors)\ndf['MA200'] = df['Close'].rolling(window=200).mean()\n\n# Drop rows with NaN values (the first 200 days won't have an average)\ndf.dropna(inplace=True)\n\n# Plotting the strategy chart\nplt.figure(figsize=(16,8))\nplt.title('S&P 500: Price vs. Moving Averages (50 & 200 days)')\nplt.plot(df['Close'], label='Closing Price', alpha=0.5)\nplt.plot(df['MA50'], label='50-Day MA', color='orange')\nplt.plot(df['MA200'], label='200-Day MA', color='red')\nplt.xlabel('Date')\nplt.ylabel('Price USD ($)')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:14.944037Z","iopub.execute_input":"2026-02-25T16:02:14.944747Z","iopub.status.idle":"2026-02-25T16:02:15.232344Z","shell.execute_reply.started":"2026-02-25T16:02:14.944706Z","shell.execute_reply":"2026-02-25T16:02:15.231295Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 3: Target Labeling for Machine Learning\n\nTo train a **Binary Classification** model, we need a target variable. \nWe will create a 'Target' column where:\n* **1 (Up):** If tomorrow's closing price is higher than today's.\n* **0 (Down):** If tomorrow's closing price is lower than today's.\n\nWe use the `.shift(-1)` method to align tomorrow's price with today's features.","metadata":{}},{"cell_type":"code","source":"# Create a column for tomorrow's price\n# We use .iloc[:, 0] if 'Close' is still being treated as a single-column DataFrame\ndf['Tomorrow'] = df['Close'].shift(-1)\n\n# Create the Target: 1 if price goes up, 0 if it goes down\n# We use .values to compare only the numbers, avoiding index alignment issues\ndf['Target'] = (df['Tomorrow'].values > df['Close'].values).astype(int)\n\n# Check the results\ndf[['Close', 'Tomorrow', 'Target']].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:15.233661Z","iopub.execute_input":"2026-02-25T16:02:15.234092Z","iopub.status.idle":"2026-02-25T16:02:15.248374Z","shell.execute_reply.started":"2026-02-25T16:02:15.234052Z","shell.execute_reply":"2026-02-25T16:02:15.247114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# It's important to check if the data is balanced (roughly 50/50 split)\nprint(\"Target Distribution:\")\nprint(df['Target'].value_counts(normalize=True))\n\n# Visualizing the distribution\nsns.countplot(x='Target', data=df)\nplt.title('Target Distribution (0: Down, 1: Up)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:15.249707Z","iopub.execute_input":"2026-02-25T16:02:15.250062Z","iopub.status.idle":"2026-02-25T16:02:15.416863Z","shell.execute_reply.started":"2026-02-25T16:02:15.250004Z","shell.execute_reply":"2026-02-25T16:02:15.415860Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 4: Feature Engineering - Adding Market Context\n\nTo help the model identify patterns, we need to provide more than just the closing price. We will add:\n1. **RSI (Relative Strength Index):** Measures if the market is overbought (>70) or oversold (<30).\n2. **Daily Returns:** Percentage change in price from the previous day.\n3. **Volatility:** The standard deviation of prices over a 10-day window.\n4. **Volume Gap:** Percentage change in trading volume.\n5. **HL_Range:** The spread between the day's High and Low price.","metadata":{}},{"cell_type":"code","source":"# 1. RSI (Relative Strength Index) - 14 periods\ndelta = df['Close'].diff()\ngain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\nloss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\nrs = gain / loss\ndf['RSI'] = 100 - (100 / (1 + rs))\n\n# 2. Daily Returns (%)\ndf['Daily_Return'] = df['Close'].pct_change()\n\n# 3. Volatility (10-day rolling standard deviation)\ndf['Volatility'] = df['Close'].rolling(window=10).std()\n\n# 4. Volume Change (%)\ndf['Volume_Change'] = df['Volume'].pct_change()\n\n# 5. High-Low Range (Daily spread)\ndf['HL_Range'] = (df['High'] - df['Low']) / df['Close']\n\n# Drop the new NaN values created by these calculations\ndf.dropna(inplace=True)\n\n# Display the updated dataframe with the new features\ndf[['Close', 'RSI', 'Daily_Return', 'Volatility', 'Volume_Change', 'HL_Range']].tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:15.418124Z","iopub.execute_input":"2026-02-25T16:02:15.418396Z","iopub.status.idle":"2026-02-25T16:02:15.446555Z","shell.execute_reply.started":"2026-02-25T16:02:15.418371Z","shell.execute_reply":"2026-02-25T16:02:15.445399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Feature Correlation\n\nBefore training, it's useful to see how our new features relate to each other. A Heatmap helps us identify if some features are too similar (multicollinearity) or have a visible relationship with the Target.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n# We only correlate numeric features and the Target\ncorrelation_matrix = df[['Close', 'MA50', 'MA200', 'RSI', 'Daily_Return', 'Volatility', 'Volume_Change', 'HL_Range', 'Target']].corr()\n\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Feature Correlation Heatmap')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:15.447778Z","iopub.execute_input":"2026-02-25T16:02:15.448275Z","iopub.status.idle":"2026-02-25T16:02:15.891881Z","shell.execute_reply.started":"2026-02-25T16:02:15.448235Z","shell.execute_reply":"2026-02-25T16:02:15.890849Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 5: Training the Random Forest Model\n\nIn financial time series, we **cannot** use random shuffling for our training and testing sets, as this would cause \"look-ahead bias\" (using future data to predict the past). \n\nInstead, we will:\n1. Split the data chronologically (using the first 80% for training and the last 20% for testing).\n2. Use a **Random Forest Classifier** with 200 estimators.\n3. Evaluate the results using a **Confusion Matrix** and **Classification Report**.","metadata":{}},{"cell_type":"code","source":"# 1. Replace infinite values with NaN\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# 2. Drop all rows that contain any NaN (including the ones we just created)\ndf.dropna(inplace=True)\n\n# 3. Sanity Check: Ensure there are no more NaNs or Infs\nprint(\"Are there any NaNs left?\", df.isnull().values.any())\nprint(\"Are there any Infs left?\", np.isinf(df.to_numpy()).any())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:15.894787Z","iopub.execute_input":"2026-02-25T16:02:15.895141Z","iopub.status.idle":"2026-02-25T16:02:15.907431Z","shell.execute_reply.started":"2026-02-25T16:02:15.895111Z","shell.execute_reply":"2026-02-25T16:02:15.906317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Selecting our features and target\nfeatures = ['Close', 'MA50', 'MA200', 'RSI', 'Daily_Return', 'Volatility', 'Volume_Change', 'HL_Range']\nX = df[features]\ny = df['Target']\n\n# Chronological split (last 20% for testing)\nsplit_index = int(len(df) * 0.8)\nX_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\ny_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n\nprint(f\"Training samples: {len(X_train)} | Testing samples: {len(X_test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:15.908751Z","iopub.execute_input":"2026-02-25T16:02:15.909168Z","iopub.status.idle":"2026-02-25T16:02:15.926556Z","shell.execute_reply.started":"2026-02-25T16:02:15.909129Z","shell.execute_reply":"2026-02-25T16:02:15.925513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initializing the model\n# n_estimators=200 means we use 200 decision trees\n# min_samples_split=100 helps prevent overfitting\nmodel = RandomForestClassifier(n_estimators=200, min_samples_split=100, random_state=1)\n\n# Training the model\nmodel.fit(X_train, y_train)\n\n# Making predictions\npreds = model.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:15.927735Z","iopub.execute_input":"2026-02-25T16:02:15.928080Z","iopub.status.idle":"2026-02-25T16:02:17.476454Z","shell.execute_reply.started":"2026-02-25T16:02:15.928043Z","shell.execute_reply":"2026-02-25T16:02:17.475363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculating Accuracy\nacc = accuracy_score(y_test, preds)\nprint(f\"Model Accuracy: {acc:.2%}\")\n\n# Confusion Matrix Visualization\nplt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, preds), annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix: Predicted vs Actual')\nplt.xlabel('Predicted (0=Down, 1=Up)')\nplt.ylabel('Actual (0=Down, 1=Up)')\nplt.show()\n\nprint(classification_report(y_test, preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:17.477484Z","iopub.execute_input":"2026-02-25T16:02:17.477757Z","iopub.status.idle":"2026-02-25T16:02:17.668939Z","shell.execute_reply.started":"2026-02-25T16:02:17.477732Z","shell.execute_reply":"2026-02-25T16:02:17.667904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting which features influenced the model the most\nimportances = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n\nplt.figure(figsize=(10,6))\nimportances.plot(kind='bar')\nplt.title('Feature Importance (What the AI learned most from)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:17.670158Z","iopub.execute_input":"2026-02-25T16:02:17.670533Z","iopub.status.idle":"2026-02-25T16:02:17.888366Z","shell.execute_reply.started":"2026-02-25T16:02:17.670505Z","shell.execute_reply":"2026-02-25T16:02:17.887267Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 7: Why Accuracy isn't Everything (Precision & Logic)\n\nIn trading, we don't need to be right every time. We need to be right when we decide to actually enter a trade. \n\n* **Precision:** When the model predicted \"UP\", how often was it actually \"UP\"?\n* **Backtesting:** Let's simulate a simple strategy: if the model predicts a rise, we \"buy\" the index.","metadata":{}},{"cell_type":"code","source":"# 1. Ensure we are using the correct variables from the 48% model\n# If your predictions were saved as 'preds', use them here\ncurrent_preds = preds \n\n# 2. Calculate Precision\nfrom sklearn.metrics import precision_score\nprec = precision_score(y_test, current_preds)\n\n# 3. Create the Backtest DataFrame\n# We use .copy() to avoid warnings and ensure the index matches y_test\nbacktest_df = pd.DataFrame(index=y_test.index)\nbacktest_df['Actual_Target'] = y_test.values\nbacktest_df['Predictions'] = current_preds\n\n# 4. Get the Actual Returns from the original dataframe\n# This is where the error often happens, so we use .loc to be precise\nbacktest_df['Actual_Returns'] = df['Daily_Return'].loc[y_test.index].values\n\n# 5. Calculate Strategy Returns\n# If the model predicts 1 (Up), we take the market return. If 0, we stay out (0 return).\nbacktest_df['Strategy_Returns'] = backtest_df['Predictions'] * backtest_df['Actual_Returns']\n\n# 6. Calculate Cumulative Growth (Starting from $1)\nbacktest_df['Cumulative_Market'] = (1 + backtest_df['Actual_Returns']).cumprod()\nbacktest_df['Cumulative_Strategy'] = (1 + backtest_df['Strategy_Returns']).cumprod()\n\nprint(f\"Model Precision (When it says 'Up', how often it's right): {prec:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:17.889516Z","iopub.execute_input":"2026-02-25T16:02:17.889800Z","iopub.status.idle":"2026-02-25T16:02:17.910601Z","shell.execute_reply.started":"2026-02-25T16:02:17.889773Z","shell.execute_reply":"2026-02-25T16:02:17.909371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.title('Performance: AI Strategy vs. S&P 500 Buy & Hold')\nplt.plot(backtest_df['Cumulative_Market'], label='S&P 500 (Market)', color='gray', alpha=0.5)\nplt.plot(backtest_df['Cumulative_Strategy'], label='AI Strategy', color='green', linewidth=2)\nplt.xlabel('Date')\nplt.ylabel('Growth of $1')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:02:17.911874Z","iopub.execute_input":"2026-02-25T16:02:17.912566Z","iopub.status.idle":"2026-02-25T16:02:18.208115Z","shell.execute_reply.started":"2026-02-25T16:02:17.912470Z","shell.execute_reply":"2026-02-25T16:02:18.206797Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final Conclusion ðŸ\n\n### 1. The \"Quant\" Edge\nWhile the cumulative return of this initial model underperformed the **Buy & Hold** strategy, we achieved a **Precision of 55% on 'UP' signals**. In quantitative finance, a consistent 5% edge over random chance is highly significant.\n\n### 2. Why Buy & Hold won?\nThe S&P 500 is inherently biased to the upside. By staying in \"Cash\" during the model's 'Down' predictions, we missed some recovery rallies. This is a classic trade-off between **Risk Mitigation** and **Opportunity Cost**.\n\n### 3. Future Improvements\nTo move this from a 55% precision to a profitable trading system, I would:\n* Implement **Sentiment Analysis** (NLP) from financial news.\n* Use **Hyperparameter Tuning** (Optuna) to refine the Random Forest.\n* Add **Macroeconomic Indicators** (Interest rates, Inflation) as features.\n\n**This notebook demonstrates a full end-to-end Machine Learning pipeline: from data ingestion and cleaning to feature engineering and backtesting.**","metadata":{}}]}